{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "# Loading Data\n",
    "\n",
    "X= digits.data\n",
    "y= digits.target\n",
    "\n",
    "# standardize\n",
    "X = (X - X.mean())/X.std()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437,), (360,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "\n",
    "    def __init__(self, lr=0.01, epochs=50,\n",
    "                 l2=0.01,\n",
    "                 minibatches=1,\n",
    "                 n_classes=None,\n",
    "                 random_seed=None):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.minibatches = minibatches\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n",
    "                     scale=0.01, random_seed=None):\n",
    "        \"\"\"Initialize weight coefficients.\"\"\"\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(shape=bias_shape)\n",
    "        return b.astype(dtype), w.astype(dtype)\n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "\n",
    "        if self.random_seed is not None:\n",
    "            np.random.seed(self.random_seed)\n",
    "        self._fit(X=X, y=y, init_params=init_params)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self._to_classlabels(probas)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if not self._is_fitted:\n",
    "            raise AttributeError('Model is not fitted, yet.')\n",
    "        return self._predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        net = self.hx(X, self.weights, self.bias)\n",
    "        pred = self._softmax(net)\n",
    "        return pred\n",
    "\n",
    "    def hx(self, X, W, b):\n",
    "        return (X.dot(W) + b)\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def _cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def _cost(self, cross_entropy):\n",
    "        L2_term = self.l2 * np.sum(self.weights ** 2)\n",
    "        cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "    def _to_classlabels(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "\n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)\n",
    "\n",
    "    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n",
    "        indices = np.arange(data_ary.shape[0])\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "        if n_batches > 1:\n",
    "            remainder = data_ary.shape[0] % n_batches\n",
    "\n",
    "            if remainder:\n",
    "                minis = np.array_split(indices[:-remainder], n_batches)\n",
    "                minis[-1] = np.concatenate((minis[-1],\n",
    "                                            indices[-remainder:]),\n",
    "                                           axis=0)\n",
    "            else:\n",
    "                minis = np.array_split(indices, n_batches)\n",
    "\n",
    "        else:\n",
    "            minis = (indices,)\n",
    "\n",
    "        for idx_batch in minis:\n",
    "            yield idx_batch\n",
    "\n",
    "    def _shuffle_arrays(self, arrays):\n",
    "        \"\"\"Shuffle arrays in unison.\"\"\"\n",
    "        r = np.random.permutation(len(arrays[0]))\n",
    "        return [ary[r] for ary in arrays]\n",
    "\n",
    "    def _fit(self, X, y, init_params=True):\n",
    "        if init_params:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = np.max(y) + 1\n",
    "            self._n_features = X.shape[1]\n",
    "\n",
    "            self.bias, self.weights = self._init_params(\n",
    "                weights_shape=(self._n_features, self.n_classes),\n",
    "                bias_shape=(self.n_classes,),\n",
    "                random_seed=self.random_seed)\n",
    "            self.cost_ = []\n",
    "\n",
    "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
    "        \n",
    "        self.avg_weight=0\n",
    "        self.avg_bias =0\n",
    "        self.beta=0.9\n",
    "        \n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for idx in self._yield_minibatches_idx(\n",
    "                    n_batches=self.minibatches,\n",
    "                    data_ary=y,\n",
    "                    shuffle=True):\n",
    "\n",
    "                z = self.hx(X[idx], self.weights, self.bias)\n",
    "                pred = self._softmax(z)\n",
    "                diff = pred - y_enc[idx]\n",
    "\n",
    "                # gradient -> n_features x n_classes\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "\n",
    "                \n",
    "                ## Momentum \n",
    "                self.avg_weight = self.avg_weight * self.beta + (1.0 - self.beta) * grad\n",
    "                self.avg_bias = self.avg_bias * self.beta + (1.0- self.beta) * grad\n",
    "                \n",
    "                # update in opp. direction of the cost gradient\n",
    "                self.weights -= (self.lr * self.avg_weight + self.lr * self.l2 * self.weights)\n",
    "                self.bias -= (self.lr * np.sum(diff, axis=0))\n",
    "                \n",
    "                \n",
    "                # decay learning rate\n",
    "                self.lr = self.lr * 0.95\n",
    "\n",
    "            # compute cost of the whole epoch\n",
    "            z = self.hx(X, self.weights, self.bias)\n",
    "            pred = self._softmax(z)\n",
    "            cross_ent = self._cross_entropy(output=pred, y_target=y_enc)\n",
    "            cost = self._cost(cross_ent)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (1437,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model test accuracy is :  0.9361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "\n",
    "X= digits.data\n",
    "y= digits.target\n",
    "\n",
    "\n",
    "# standardize\n",
    "X = (X - X.mean())/X.std()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "multiClassLogReg = SoftmaxRegression(lr=0.01, epochs=10, minibatches=1, random_seed=0)\n",
    "multiClassLogReg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = multiClassLogReg.predict(X_test)\n",
    "print(\"model test accuracy is : \",  accuracy_score(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiClassLogReg = SoftmaxRegression(lr=0.01, epochs=800, minibatches=1, random_seed=0)\n",
    "multiClassLogReg.fit(X_train, y_train)\n",
    "y_pred = multiClassLogReg.predict(X_test)\n",
    "tr_y_pred = multiClassLogReg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Gradient Descent model test accuracy is :  0.9638888888888889\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*25)\n",
    "print(\"Gradient Descent model test accuracy is : \",accuracy_score(y_test,y_pred )  )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Gradient Descent model train accuracy is :  0.9798190675017397\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*25)\n",
    "print(\"Gradient Descent model train accuracy is : \",accuracy_score(y_train,tr_y_pred)  )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "X= digits.data\n",
    "y= digits.target\n",
    "\n",
    "# standardize\n",
    "X = (X - X.mean())/X.std()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "lr = SoftmaxRegression(lr=0.05, epochs=200, minibatches=50, random_seed=0)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "tr_y_pred = multiClassLogReg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SGD model test accuracy is :  0.95\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*25)\n",
    "print(\"SGD model test accuracy is : \",accuracy_score(y_test,y_pred )  )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SGD model train accuracy is :  0.9798190675017397\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*25)\n",
    "print(\"SGD model train accuracy is : \",accuracy_score(y_train,tr_y_pred)  )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/pranay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SK Learn Logistic Regression model test accuracy is :  0.9611111111111111\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logRegres = LogisticRegression()\n",
    "\n",
    "logRegres.fit(X_train, y_train)\n",
    "\n",
    "sk_y_pred = logRegres.predict(X_test)\n",
    "\n",
    "print(\"==\"*25)\n",
    "print(\"SK Learn Logistic Regression model test accuracy is : \",  accuracy_score(y_test,sk_y_pred) )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SK Learn Logistic Regression model train accuracy is :  0.988865692414753\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sk_tr_y_pred = logRegres.predict(X_train)\n",
    "\n",
    "print(\"==\"*25)\n",
    "print(\"SK Learn Logistic Regression model train accuracy is : \",  accuracy_score(y_train,sk_tr_y_pred) )\n",
    "print(\"==\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+-----------+---------+---------+------+\n",
      "|   Metric  | Sklearn |    GD   | SGD  |\n",
      "+-----------+---------+---------+------+\n",
      "| Accuracy  | 0.96111 | 0.96388 | 0.95 |\n",
      "+-----------+---------+---------+------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names =  ['Metric','Sklearn','GD','SGD']\n",
    "x.add_row([\"Accuracy \", 0.96111,0.96388,0.95])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referernce :\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/softmax-regression.ipynb\n",
    "- https://github.com/python-engineer/MLfromscratch/tree/master/mlfromscratch\n",
    "- https://engmrk.com/gradient-descent-with-momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mathematical terms:\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
